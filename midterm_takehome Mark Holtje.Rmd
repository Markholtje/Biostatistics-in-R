---
title: "Midterm takehome exam Spring 2021"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

Name: Mark Holtje


Please answer the following questions and upload the answer as an R markdown file and corresponding html file.
All questions are equal marks.

Q1.  Answer the following questions.

Define the standard error of a statistic.

  Answer: The standard error of a statistic (or an estimate of a parameter) refers to the standard deviation of its sampling distribution, or the estimate of that standard deviation. The standard error measures the accuracy with which a sample distribution represents a population by using standard deviation. It is calculated as the standard deviation (σ) divided by the square root of the number of samples. 


Define "p-value" in conditional probability notation.   

   Answer: the "p-value" in conditional probability notation refers to the conditional probability of observing the observed data (plus other data that is at least as extreme as that observed) given that the null hypothesis (Ho) is true. For example, a very small p-value signifies that such an extreme observed outcome would be very unlikely under the null hypothesis. Pr{value this (or more) extreme | null hypothesis}


Your colleague calculated a p-value of 0.05 for an experiment and states that this means that there is only a 5% chance that the null hypothesis is true. Is this correct? Why or why not? Approximately, what would you expect the posterior probability of the null being true to be?

  Answer: Given a p-value of 0.05, this means that there is an observed effect. This is a statistically significant test result since P ≤ 0.05, and so the null hypothesis is false and should be rejected. Since our colleague calculated a p-value is 0.05, that means that 5% of the time there would be a test statistic at least as extreme as the one he/she found if the null hypothesis was true. 


Define "95% confidence interval". 

  Answer: The confidence interval (CI) is a statistical estimate of the observed data. It provides a range of values for an unknown parameter (i.e. the mean). This interval has an associated confidence level that provides the probability with which the estimated interval will contain the true parameter. For a given estimation in a given sample, using a higher confidence level generates a wider (i.e., less precise) confidence interval. This means that the confidence level represents the theoretical long-run frequency (i.e., the proportion) of confidence intervals that contain the true value of the unknown population parameter. In other words, a 95% confidence interval refers to 95% of confidence intervals computed at the 95% confidence level contain the parameter. 


Name two compute-intensive methods you could use to calculate a p-value? Describe their main steps.    

  Answer: Two compute-intensive methods you could use to calculate a p-value are the Permutation test and the Monte Carlo Simulation. The Monte Carlo simulation utilizes a large amount of random simulations based on observed data, whereas the Permutation test builds a sampling distribution by shuffling the observed data without replacement.


Name a compute-intensive method you could use to estimate a 95% confidence interval. Describe its main steps.

  Answer: A compute-intensive method you could use to estimate a 95% confidence interval would be the bootstrap method since it makes no distributional assumptions, and it simulates generating a set of samples from the population by re-sampling our single sample, with replacement. The main steps would be to loop 100 times by taking a sample each time, then taking the median (just as an example). Afterwards, you would find the square root of the variance of all 100 of the medians, in other words, the standard deviation of the 100 recorded medians. You would then visualize the medians using a histogram, and analyze the histogram for distribution of data. You might then also want to calculate the confidence interval value range (the example in the slides used 3308-3511), as well as the estimated standard error of the mean. 
  

According to the American Statistical Association (ASA) statement on p-values (2016), is the following statement true or false? Why?
  [A p-value, or statistical significance, measures the size of an effect or the importance of a result.]
  
  Answer: The statement of the American Statistical Association (ASA) statement on p-values (2016), is false, since apparently with regards to p-values, they tend to deflect attention from the actual size of an effect. P-values signify the conditional probability of observing the observed data (plus other data that is at least as extreme as that observed) given that the null hypothesis (Ho) is true, but would not discuss the size of an effect or the importance of a result. 
  
Q2. In a clinic, the systolic blood pressures in mmHg of a random sample of 10 patients with a certain metabolic disorder were collected. Assume blood pressure is normally-distributed in the population. The mean of this sample was 103.2 mmHg with a (sample) standard deviation of 15.0 mmHg.  Test the hypothesis that the mean blood pressure of this sample of patients differs from the known population mean systolic blood pressure of 121.2 mmHg.

Show your working including null hypothesis, alternative hypothesis, test statistic and p-value, and interpret your p-value. 
Also give a 95% confidence interval.
What experimental design is this?
What do we mean by "a random sample" in the question?

If the population standard deviation was actually known to be 15.0 mmHg exactly (from previous large studies), compute your p-value in this case. 

  Answer: The null hypothesis is that the mean blood pressure of this sample of patients does not differ from the known population mean systolic blood pressure of 121.2 mmHg.

The alternate hypothesis is that the mean blood pressure of this sample of patients differs from the known population mean systolic blood pressure of 121.2 mmHg.

Test statistic: Z = (x_bar - µ)/(S/sqrt(n)) or (103.2-121.2)/(15.0/sqrt(10)) or -3.795

```{r}
z <- -3.795
p_val <- 2*(1-pt(z, df=9))
p_val
```

The P-value is 1.99575, which means that there is not strong evidence against the null hypothesis, which means that it is not safe to reject the null hypothesis. Therefore, the mean blood pressure of this sample of patients does not differ from the known population mean systolic blood pressure of 121.2 mmHg. 


```{r}
sample_mean <- 103.2
std_dev <- 15
sample_size <- 10
CI_95_val <- 1.96

lower_bound <- sample_mean - CI_95_val * (std_dev/sqrt(sample_size)) 
higher_bound <- sample_mean + CI_95_val * (std_dev/sqrt(sample_size))
lower_bound
higher_bound
```
The 95% confidence interval value range is 93.902 mmHg to 112.497 mmHg systolic blood pressure.

The experimental design is a one-sample t-test, which is a type of inferential statistic used to examine whether the mean of a population is statistically different from a known or hypothesized value. 

What we mean by "a random sample" is a sample taken from the population in which each sample has an equal probability of being chosen. A sample chosen randomly is meant to be an unbiased representation of the total population. In this case, the systolic blood pressures in mmHg of a random sample of 10 patients with a certain metabolic disorder were collected from a population where blood pressure is normally-distributed. 

If the population standard deviation was actually known to be 15.0 mmHg exactly (from previous large studies), this is the computed p-value: Z = (x_bar - µ)/(σ/sqrt(n)) or (103.2-121.2)/(15.0/sqrt(10)) or -3.795 as test-stat.
```{r}
z <- -3.795
p_val <- 2*(1-pt(z, df=9))
p_val
```
The P-value is also 1.99575.


Q3. In a large community surrounding a nuclear power plant in the state of Washington, before the plant was constructed twenty years ago, the mean number of congenital malformations had been 25.0 per year. This number was consistent with the rate for the entire state. Although there has not been any change in the state-wide rate of congenital malformations, 35 congenital malformations were recorded in 2018 in the community surrounding the nuclear plant. Assume that the random number of congenital malformations is a Poisson random variable. 
(a) What would be the standard deviation of the number of congenital malformations twenty years ago? 

Since in Poisson Distribution: mean = µ = σ^2, then sqrt(25) = 5, which is the standard deviation of the number of congenital malformations twenty years ago. 


(b) What is the probability of 35 or more congenital malformations? 

```{r}
ppois(34, lambda=25, lower.tail=FALSE) #upper tail
```

(c) What is the p-value for the null hypothesis that the prevailing mean number of congenital malformations in 2018, in the community surrounding the nuclear power plant, was the same as that for the state at large. Show your null hypothesis and give any R commands you used to answer these questions. 

The null hypothesis is that there is no difference between the prevailing mean number of congenital malformations in 2018 and the state at large. 

```{r}
1-ppois(35,lambda=25,log = FALSE)
```

Given a p-value of 0.02245809, you should reject the null hypothesis, which means that is a difference between the prevailing mean number of congenital malformations in 2018 and the state at large. 


Q4. "error_bars.pdf" shows three different plots with error bars for an estimate of the mean from some sample of data. The figure legend does not indicate what the error bars mean, but it is known that they show (i)the SEM, (ii) the 95% CI, and (iii) the sample standard deviation. Which error bar is which? Which of these types of error bar would be appropriate in such a plot showing an estimate of the population mean?

Answer: The error bar for Plot A represents the sample standard deviation because it spans a moderate distance and it would be divided by degrees of freedom, whereas the error bar for Plot B represents Standard error mean (SEM) because the SEM is calculated as the sample std dev divided by the square root of the sample size, and the error bar for Plot C represents the 95% CI because the lower and higher bounds of the error bars represent the range that the values fall within.  

The type of error bar that would be appropriate in such a plot showing an estimate of the population mean would be the error bar for Plot A, because the mean of the sampling distribution of x̄ is equal to the population mean, μ. 


Q5.In the file "HW3_bootstrap.tsv" is a sample drawn from some distribution that is unknown. Compute the trimmed mean of this sample (with a trim of 5%).
What is the standard error and 95% CI of this statistic? Use a compute-intensive method. (Show R code which will involve a loop).

```{r}
my.data_trim_mean<-read.table("HW3_bootstrap.tsv", header=T)
colnames(my.data_trim_mean)
sorted_vals <- c(sort(my.data_trim_mean$X3.28008630436596))
n <- length(sorted_vals)

floor(0.05 * n) #We need to remove the bottom two values and the top two values. 

modified_sorted_vals <- sorted_vals[3:(n-2)] #trimming the actual list
length(modified_sorted_vals) #length is now correct

trim_mean <- mean(sorted_vals,trim=0.05)#This is the trimmed mean of this sample (with a trim of 5%).
trim_mean
trim_sd <- sd(sorted_vals)
trim_sd
```
The standard error and 95% CI of this statistic will be calculated by the Bootstrap compute-intensive method using a loop. 

```{r}
m <- 10000
res <- numeric(m)

for (i in 1:m) {
res[i] <- mean(sample(sorted_vals,replace=T))
 }

std_error <- function(x) sd(x)/sqrt(length(x))
std_error(res) #Standard Error calculated, which is 0.001777434

bootstrap_CI_95_val <- 1.96
mean_bootstrap <- mean(res)
mean_bootstrap #Mean of 1.809421
sd_bootstrap <- sd(res)
sd_bootstrap #Standard deviation of 0.1777434
n_bootstrap <- length(res)
n_bootstrap

boostrap_lower_bound <- mean_bootstrap - bootstrap_CI_95_val  * (sd_bootstrap/sqrt(n_bootstrap)) 
boostrap_higher_bound <- mean_bootstrap + bootstrap_CI_95_val  * (sd_bootstrap/sqrt(n_bootstrap))
boostrap_lower_bound #Lower bound of 1.805937
boostrap_higher_bound #Upper bound of 1.812905
```


Q6. The systolic blood pressure for a random sample of mice was recorded (see "blood_pressure.csv"). Blood pressure is known to be normally distributed. We want to test whether this group of mice is from a normo-tensive population (i.e. with a population expected systolic blood pressure of 120.0 mmHg).

Compute a 95% confidence interval using a parametric approach. (do not use any R ".test()" functions)

Compare with the 95% confidence interval using a compute-intensive approach.

  
  Answer: The null hypothesis is that this group of mice is not from a normo-tensive population (i.e. with a population expected systolic blood pressure of 120.0 mmHg). The alternative hypothesis is that this group of mice is from a normo-tensive population (i.e. with a population expected systolic blood pressure of 120.0 mmHg)
  
```{r}
my.data_blood_pressure<-read.table("blood_pressure.csv", header=T)
colnames(my.data_blood_pressure)
BP_vals <- c(my.data_blood_pressure$BP)
n_BP <- length(BP_vals) #Number of blood pressure values

BP_sample_mean <- mean(BP_vals) #Sample mean BP value of 131.9052 mmHg using parametric approach
BP_sample_sd <- sd(BP_vals) #Sample standard deviation BP value of 9.132537 mmHg using parametric approach
BP_95_val <- 1.96
BP_population_mean <- 120 

BP_test_stat <- (BP_sample_mean - BP_population_mean)/(BP_sample_sd/sqrt(n_BP)) #Computed Test statistic: 5.829907
BP_test_stat
BP_P_val <- 2*(1-pt(BP_test_stat, df = n_BP-1)) #Computed P-value of 1.290706e-05
BP_P_val
```

The P-value is 0.00001290706, making it statistically significant. This means that there is strong evidence against the null hypothesis, which means that it is safe to reject the null hypothesis. Therefore, this group of mice is from a normo-tensive population (population expected systolic blood pressure of 120.0 mmHg). 


Parametric approach to find Upper and lower bound for 95% CI: 
```{r}
BP_lower_bound <- BP_sample_mean - BP_95_val * (BP_sample_sd/sqrt(n_BP)) 
BP_higher_bound <- BP_sample_mean + BP_95_val * (BP_sample_sd/sqrt(n_BP))
BP_lower_bound #Lower bound BP value of 127.9027 mmHg using parametric approach
BP_higher_bound #Upper bound BP value of 135.9077 mmHg using parametric approach
```

Compute-Intensive Approach (Bootstrap) to find Upper and lower bound for 95% CI: 
```{r}

m_BP <- 100
res_BP <- numeric(m_BP)

for (i in 1:m_BP) {
res_BP[i] <- median(sample(BP_vals,replace=T))
 }

mean_bootstrap_BP <- mean(res_BP) 
mean_bootstrap_BP #Mean BP of 132.944 mmHg after bootstrap test
sd_bootstrap_BP <- sd(res_BP)
sd_bootstrap_BP  #Standard deviation of 2.320686 mmHg BP after bootstrap test
n_bootstrap_BP <- length(res_BP)
n_bootstrap_BP    #Length of 100 values for bootstrap test

boostrap_lower_bound_BP <- mean_bootstrap_BP - bootstrap_CI_95_val  * (sd_bootstrap_BP/sqrt(n_bootstrap_BP)) 
boostrap_higher_bound_BP <- mean_bootstrap_BP + bootstrap_CI_95_val  * (sd_bootstrap_BP/sqrt(n_bootstrap_BP))
boostrap_lower_bound_BP  #Lower bound BP value of 132.7924 mmHg using Bootstrap compute-intensive approach
boostrap_higher_bound_BP #Upper bound BP value of 133.5451 mmHg using Bootstrap compute-intensive approach
```

For comparison: There is  a mean blood pressure (BP) of 132.944 mmHg and a standard deviation of 2.320686 mmHg after conducting a bootstrap test. There is also a lower bound BP value of 132.7924 mmHg and an upper bound BP value of 133.5451 mmHg. Whereas, for the parametric approach, there is a sample mean BP value of 131.9052 mmHg, a sample standard deviation BP value of 9.132537 mmHg, a lower bound BP value of 127.9027 mmHg, and an upper bound BP value of 135.9077 mmHg. 


Q7. In this exercise, we’re going to take advantage of R’s simulation functions to empirically test how well confidence intervals hold up.
Create a loop of M=10000 iterations that draws a sample size of n=5 random numbers from a normal distribution each time. Let this normal distribution be defined by population parameters of mean (mu) = 2.0 and sigma (standard deviation) = 2.0. At each iteration, estimate the sample mean, estimate the standard error of the mean (SEM) and from that calculate a 95% confidence interval using the true population standard deviation (which is of course known exactly in this simulation). 

What R distribution function do you use here and why?

In your code, calculate the following:
1. Compute the fraction of iterations in which the true population mean (mu) is within the bounds of your calculated 95% confidence interval.
2. Explain whether the value you calculated is what you expected. 

Then repeat using exactly the same code (using a normal distribution in your CI calculation), but this time using the standard deviation estimated from each sample rather than the known population standard deviation for the SEM and 95% CI (which is more realistic). 
This calculation is not quite correct- what do you notice about the fraction overlapping in this case, and why do you get that result?

Then repeat the code, again using the estimated sample standard deviation but now using the correct formulation of 95% CI (using a t-distribution). What fraction overlaps now?


You may find these tutorial descriptions useful:
http://www.zoology.ubc.ca/~whitlock/Kingfisher/SamplingNormal.htm
http://www.zoology.ubc.ca/~whitlock/Kingfisher/CIMean.htm

```{r}
monte_carlo_sim_1 <- c()
for (i in 1:10000) { 
  monte_carlo_sim_1[i] <- mean(rnorm(5,mean=2, sd=2))
}; 

hist(monte_carlo_sim_1, xlim=c(-2,6)) #Histogram of the Monte Carlo Simulation
sd(monte_carlo_sim_1) #Computed SEM of 0.8991999
pop_mean <- mean(monte_carlo_sim_1)
pop_mean # tends to mean of population (law of large numbers) = 1.986581
par(mfrow=c(1,1))

#95% CI: 

length(monte_carlo_sim_1)
monte_carlo_sim_1_sorted <- sort(monte_carlo_sim_1)
monte_carlo_sim_1_sorted[0.025*10000]  # 95% CI lower bound: 0.2491352
monte_carlo_sim_1_sorted[0.975*10000]  # 95% CI upper bound: 3.730274


#Calculating fraction of iterations in which the true population mean (mu) is within the bounds of the calculated 95% confidence interval:

counter_mc_sim = 0
for (i in 1:10000) { 
  if(monte_carlo_sim_1_sorted[i] >= 0.2491352 & monte_carlo_sim_1_sorted[i] <= 3.730274) {
        counter_mc_sim = counter_mc_sim + 1}
};
counter_mc_sim #This is the number of values that fall within the 0.2491352 to 3.730274 95% CI. 

fraction_in_CI <- counter_mc_sim/length(monte_carlo_sim_1_sorted)
fraction_in_CI
```
The R distribution function used here is rnorm because rnorm is used to simulate random variates given a specified normal distribution. 

I also calculated the fraction of iterations in which the true population mean (mu) is within the bounds of the calculated 95% confidence interval. The fraction that I found was 0.95 or 95% which is expected since this represents the 95% CI of the monte_carlo_sim_1_sorted created by the Monte Carlo Compute-intensive simulation. So, in other words, 95% of the 10000 values (9500 values) fall within the 0.2491352 to 3.730274 95% CI. 

```{r}
#Repeat using exactly the same code (using a normal distribution in your CI calculation), but this time using the standard deviation estimated from each sample rather than the known population standard deviation, for the SEM and 95% CI: 

monte_carlo_sim_2 <- c() #Sample Means
monte_carlo_sim_2_sds <- c() #Sample Standard deviations
monte_carlo_sim_2_sem <- c() #Sample SEMs
monte_carlo_sim_2_lb <- c() #Sample Lower Bounds
monte_carlo_sim_2_ub <- c() #Sample Upper Bounds

for (i in 1:10000) { 
  mcs_func <- rnorm(5,mean=2, sd=2) #rnorm test 
  monte_carlo_sim_2[i] <- mean(mcs_func) #Sample Mean
  monte_carlo_sim_2_sds[i] <- sd(mcs_func) #Sample standard deviation
  monte_carlo_sim_2_sem[i] <- monte_carlo_sim_2_sds[i]/sqrt(5) #Computed Sample Standard Error of Mean(SEM)
  monte_carlo_sim_2_lb[i] <- monte_carlo_sim_2[i] - CI_95_val * (monte_carlo_sim_2_sem[i])# 95% CI lower bound
  monte_carlo_sim_2_ub[i] <- monte_carlo_sim_2[i] + CI_95_val * (monte_carlo_sim_2_sem[i])# 95% CI upper bound
}; 

#Calculating fraction of iterations where the sample mean is within the bounds the calculated 95% confidence interval:
counter_mc_sim_2 = 0

for (i in 1:10000) { 
  if(pop_mean >= monte_carlo_sim_2_lb[i] & pop_mean <= monte_carlo_sim_2_ub[i]) {
        counter_mc_sim_2 = counter_mc_sim_2 + 1}
};
counter_mc_sim_2 #This is the number of values that fall within the 95% CI for each sample: 8756

fraction_in_CI_2 <- counter_mc_sim_2/length(monte_carlo_sim_2) #The fraction of iterations in which the sample mean is within the bounds of the calculated 95% confidence interval is ~0.8756 or 87.56%
fraction_in_CI_2
```
This calculation is not quite correct since despite being both normally distributed, the two results vary somewhat. What I noticed is that the fraction overlapping in this case is slightly lower than compared with the fraction of iterations in which the true population mean (mu) is within the bounds of the calculated 95% confidence interval.The reason is because I took into account the mean, standard deviation, Standard Error Mean, and the upper and lower bounded values of each of the samples, rather than as a true population mean (mu) When computing the CI of the samples, you have fewer overlapped sample mean values that fall within the 95% CI than with the expected 95% overlap of all of the sample mean values in a population. The reason is because if you were construct an experiment where you used the sample standard deviation, then 95 times out of 100, the true population parameter mean falls within the calculated 95%

```{r}
#Then repeat the code, again using the estimated sample standard deviation but now using the correct formulation of 95% CI (using a t-distribution). 

monte_carlo_sim_3 <- c()
monte_carlo_sim_3_sds <- c()
monte_carlo_sim_3_sem <- c()
monte_carlo_sim_3_lb <- c()
monte_carlo_sim_3_ub <- c()

qt(0.975,4) #Tabulated t-statistic given 4 df and 97.5% upper level: 2.776445

for (i in 1:10000) { 
  mcs_func_sim_3 <- rnorm(5,mean=2, sd=2) #rnom test 
  monte_carlo_sim_3[i] <- mean(mcs_func_sim_3) #Sample Mean
  monte_carlo_sim_3_sds[i] <- sd(mcs_func_sim_3) #Sample standard deviation
  monte_carlo_sim_3_sem[i] <- monte_carlo_sim_3_sds[i]/sqrt(5) #Computed Sample Standard Error of Mean (SEM)
  #t_dist_test <- (monte_carlo_sim_3[i]- mean(monte_carlo_sim_1))/(monte_carlo_sim_3_sem[i])
  monte_carlo_sim_3_lb[i] <- monte_carlo_sim_3[i] - 2.776445 * (monte_carlo_sim_3_sem[i])# 95% CI lower bound
  monte_carlo_sim_3_ub[i] <- monte_carlo_sim_3[i] + 2.776445 * (monte_carlo_sim_3_sem[i])# 95% CI upper bound
}; 

#Calculating fraction of iterations where the sample mean is within the bounds the calculated 95% confidence interval:
counter_mc_sim_3 = 0

for (i in 1:10000) { 
  if(pop_mean >= monte_carlo_sim_3_lb[i] & pop_mean <= monte_carlo_sim_3_ub[i]) {
        counter_mc_sim_3 = counter_mc_sim_3 + 1}
};
counter_mc_sim_3 #This is the number of values that fall within the 95% CI for each sample: 9504

fraction_in_CI_3 <- counter_mc_sim_3/length(monte_carlo_sim_3) #The fraction of iterations in which the sample mean is within the bounds of the calculated 95% confidence interval is 0.9504 or 95.04%
fraction_in_CI_3

#What fraction overlaps now? 

# Answer: 95.04% in this case of the Monte Carlo Simulation 
```

Q8. Define a "null distribution". How does it differ from the sampling distributions we have used for estimation of 95% confidence intervals so far in this course?
What is a null distribution used to calculate?

  Answer: A null distribution is the probability distribution of the test statistic when the null hypothesis is considered true. An example is when in an F-test, the null distribution would be an F-distribution. The null distribution is a tool that scientists often use when conducting experiments to prove that there is no statistical difference between the results of doing one thing as opposed to doing a different thing. It differs from the sampling distributions we have used for estimation of 95% confidence intervals so far in this course because to determine the distribution of two sets of data under a null hypothesis where if the results of the two sets of data are not outside the parameters of the expected results, then the null hypothesis is said to be true.

Q9. If random variables $X$ and $Y$ are normally distributed, $X \sim \text{N}(\mu=1, \sigma^2=2)$, and $Y \sim \text{N}(\mu=2, \sigma^2=3)$, fully characterize the probability density function of $Z=2X+3Y$.  

  Answer: To fully characterize the probability density function listed above, it would be Z~ N(2µ_x + 3µ_y, (2^2 * σ^2_x) + (3^2 * σ^2_y)) or Z ~ N(2*1 + 3*2, (2^2 * 2) + (3^2 * 3)), which simplified is  Z ~ N(2 + 6, (4 * 2) + (9 * 3)) or  Z ~ N(8, 35). 
  
```{r}
?pnorm
mu <- (2*1)+(3*2) #Calculate the mu based on the given µ_x and µ_y
variance <-(4*2)+(9*3) # Calculate the variance given σ^2_x and σ^2_y
dnorm(x=2, mean = mu, sd = sqrt(variance), log = FALSE) # Answer: 0.04032038
```

Q10.  Read  Nuzzo, R., Statistical Errors, Nature, Vol 506, 2014 and answer the following questions:

If you were asked "what is a p-value\?", give your response, giving both the definition in conditional probability notation as well as a prose description (answer must be less than 150 words).
How would you interpret a p-value of 0.01 in terms of its strength of evidence against the null hypothesis: what rate of false positive results might you expect if an independent lab attempted to replicate an experiment with a p-value of 0.01, and what does that depend on?

  Answer: the "p-value" in conditional probability notation refers to the conditional probability of observing the observed data (plus other data that is at least as extreme as that observed) given that the null hypothesis (Ho) is true. For example, a very small p-value signifies that such an extreme observed outcome would be very unlikely under the null hypothesis. Pr{value this (or more) extreme | null hypothesis}. Apparently with regards to p-values, it can tend to deflect attention from the actual size of an effect. 
  
  As a prose description, the p-value is to "summarize the data assuming a specific null hypothesis".
  
  Given a p-value of 0.01, I would interpret the strength of evidence against the null hypothesis as weak statistically significant test result since 0.001 ≤ P ≤ 0.05, and so the null hypothesis is false and should be rejected. Since our calculated p-value is 0.01, that means that 1% of the time there would be a test statistic at least as extreme as the one found if the null hypothesis was true. 
  
  If an independent lab attempted to replicate an experiment with a p-value of 0.01, then the rate of Type 1 error, or false positive (denoted as alpha) results that I might expect is somewhat low at around 11%. That depends on “the underlying probability that there is a true effect”. 
  
This would also depend on the lab not partaking in p-hacking: "snooping, fishing, significance-chasing, and double-dipping until you get a desired result".